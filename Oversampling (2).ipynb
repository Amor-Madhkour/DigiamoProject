{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-14T08:57:40.294981Z"},"trusted":true},"outputs":[],"source":["pip install keras-tuner --upgrade"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import seaborn as sns\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import classification_report\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Conv1D, Dense, Dropout, Bidirectional, LSTM\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras_tuner.tuners import BayesianOptimization\n","from transformers import BertTokenizer\n","import matplotlib.pyplot as plt\n","from sklearn.utils.class_weight import compute_class_weight\n","from imblearn.over_sampling import SMOTE"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df = pd.read_csv(\"/kaggle/input/japanese-newspapers-20052021/japanese_news.csv\", delimiter='\\t')\n","df['text'] = df['text'].fillna('')\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(df.head())\n","print(df.info())\n","print(df.columns)\n","\n","percentages = df['source'].value_counts(normalize=True) * 100\n","\n","plt.figure(figsize=(10, 6))\n","percentages.plot(kind='bar', color='skyblue')\n","plt.title('Percentage distribution for {}'.format(source_column))\n","plt.xlabel(source_column)\n","plt.ylabel('Percentage')\n","plt.xticks(rotation=45)\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","plt.tight_layout()\n","plt.show()\n","\n","source_counts = df['source'].value_counts()\n","\n","plt.figure(figsize=(10, 6))\n","sns.barplot(x=source_counts.index, y=source_counts.values, palette=\"viridis\")\n","plt.title('Source distribution')\n","plt.xlabel('Source')\n","plt.ylabel('Number of articles')\n","plt.xticks(rotation=45)\n","plt.show()\n","\n","\n","df['date'] = pd.to_datetime(df['date'], errors='coerce')\n","\n","invalid_dates = df[df['date'].isnull()]\n","\n","print(\"Rows with invalid dates:\")\n","print(invalid_dates)\n","\n","df = df.dropna(subset=['date'])\n","\n","df['year'] = df['date'].dt.year\n","\n","for year, year_data in df.groupby('year'):\n","    source_counts = year_data['source'].value_counts()\n","\n","    plt.figure(figsize=(10, 6))\n","    sns.barplot(x=source_counts.index, y=source_counts.values, hue=source_counts.index, palette=\"viridis\", legend=False)\n","    plt.title(f'Source distribution - Year {year}')\n","    plt.xlabel('Source')\n","    plt.ylabel('Number of articles')\n","    plt.xticks(rotation=45)\n","    plt.show()\n","    plt.tight_layout()\n","\n","df['year'] = pd.to_datetime(df['date']).dt.year\n","articles_per_year = df['year'].value_counts().sort_index()\n","\n","plt.figure(figsize=(10,6))\n","articles_per_year.plot(kind='line', marker='o')\n","plt.title('Number of articles per year')\n","plt.xlabel('Year')\n","plt.ylabel('Number of articles')\n","plt.xticks(rotation=45)\n","plt.grid(True)\n","plt.show()\n","plt.tight_layout()\n","\n","articles_per_source = df['source'].value_counts()\n","\n","plt.figure(figsize=(10,6))\n","articles_per_source.plot(kind='line', marker='o')\n","plt.title('Number of articles per source')\n","plt.xlabel('Source')\n","plt.ylabel('Number of articles')\n","plt.xticks(rotation=45)\n","plt.show()\n","plt.tight_layout()\n","\n","articles_per_year_and_source = df.groupby(['year', 'source']).size().unstack(fill_value=0)\n","\n","plt.figure(figsize=(18, 12))\n","articles_per_year_and_source.plot(kind='line', marker='o')\n","plt.title('Number of articles per year per source')\n","plt.xlabel('Year')\n","plt.ylabel('Number of articles')\n","plt.xticks(articles_per_year_and_source.index, rotation=45)\n","plt.grid(True)\n","plt.legend(title='Source', bbox_to_anchor=(1, 1))\n","plt.tight_layout()\n","plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["desired_sample_size = 51000\n","df_sample = df.sample(n=desired_sample_size, random_state=42)\n","\n","X = df_sample['text']\n","y = df_sample['source']\n","\n","num_unique_sources = df_sample['source'].nunique()\n","print(\"Number of output classes 'source':\", num_unique_sources)\n","\n","label_encoder = LabelEncoder()\n","y = label_encoder.fit_transform(y)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n","X_train_tok = tokenizer(X_train.tolist(), padding=True, truncation=True, max_length=115, return_tensors=\"tf\")\n","X_test_tok = tokenizer(X_test.tolist(), padding=True, truncation=True, max_length=115, return_tensors=\"tf\")\n","\n","max_len = 115 \n","X_train_pad = pad_sequences(X_train_tok[\"input_ids\"], maxlen=max_len, padding='post')\n","X_test_pad = pad_sequences(X_test_tok[\"input_ids\"], maxlen=max_len, padding='post')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y= y_train)\n","\n","class_weights_dict = dict(enumerate(class_weights))\n","\n","print(\"Class weights:\", class_weights_dict)\n","\n","class_weights_df = pd.DataFrame({'Class': np.unique(y_train), 'Weight': class_weights})\n","\n","print(class_weights_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sm = SMOTE(random_state=42)\n","X_train_resampled, y_train_resampled = sm.fit_resample(X_train_pad, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def build_model(hp):\n","    model = Sequential()\n","    model.add(Embedding(input_dim=len(tokenizer), output_dim=hp.Int('embedding_output_dim', min_value=100, max_value=250, step=50)))\n","    model.add(Conv1D(hp.Int('conv1d_filters', min_value=64, max_value=256, step=64), 5, activation='relu'))\n","    model.add(Bidirectional(LSTM(hp.Int('lstm_units', min_value=32, max_value=128, step=32), dropout=hp.Float('lstm_dropout', min_value=0.1, max_value=0.5, step=0.1), recurrent_dropout=hp.Float('recurrent_dropout', min_value=0.1, max_value=0.5, step=0.1))))\n","    model.add(Dense(hp.Int('dense_units', min_value=32, max_value=128, step=32), activation='relu'))\n","    model.add(Dropout(hp.Float('dropout', min_value=0.1, max_value=0.5, step=0.1)))\n","    model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n","\n","    model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])), metrics=['accuracy'])\n","    return model\n","tuner = BayesianOptimization(\n","    build_model,\n","    objective='val_accuracy',\n","    max_trials=8,  \n","    directory='my_dir',\n","    project_name='japanese_news'\n",")\n","\n","tuner.search(X_train_resampled, y_train_resampled, epochs=5, validation_data=(X_test_pad, y_test), class_weight=class_weights_dict)\n","\n","best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n","\n","print(\"Best Hyperparameters:\")\n","print(best_hps.values)\n","\n","\n","model = tuner.hypermodel.build(best_hps)\n","history = model.fit(X_train_resampled, y_train_resampled, epochs=30, batch_size=32, validation_data=(X_test_pad, y_test))\n","\n","\n","loss, accuracy = model.evaluate(X_test_pad, y_test)\n","print(\"Test Accuracy:\", accuracy)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Evaluate model\n","loss, accuracy = model.evaluate(X_test_pad, y_test)\n","print(\"Test Accuracy:\", accuracy)\n","\n","# Fai previsioni sul set di test\n","y_pred = model.predict_classes(X_test_pad)\n","\n","# Calcola l'F1-score e la matrice di confusione\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Confusion Matrix:\")\n","conf_mat = confusion_matrix(y_test, y_pred)\n","print(conf_mat)\n","# Plot confusion matrix\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix')\n","plt.show()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":2237026,"sourceId":3748403,"sourceType":"datasetVersion"}],"dockerImageVersionId":30648,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
