{"cells":[{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-02-10T16:01:32.424312Z","iopub.status.busy":"2024-02-10T16:01:32.423694Z","iopub.status.idle":"2024-02-10T16:01:45.061306Z","shell.execute_reply":"2024-02-10T16:01:45.059967Z","shell.execute_reply.started":"2024-02-10T16:01:32.424279Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: keras-tuner in /opt/conda/lib/python3.10/site-packages (1.4.6)\n","Requirement already satisfied: keras in /opt/conda/lib/python3.10/site-packages (from keras-tuner) (2.15.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from keras-tuner) (21.3)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from keras-tuner) (2.31.0)\n","Requirement already satisfied: kt-legacy in /opt/conda/lib/python3.10/site-packages (from keras-tuner) (1.0.5)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->keras-tuner) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->keras-tuner) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->keras-tuner) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->keras-tuner) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->keras-tuner) (2023.11.17)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["!pip install keras-tuner --upgrade"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2024-02-10T15:36:11.972959Z","iopub.status.busy":"2024-02-10T15:36:11.972040Z","iopub.status.idle":"2024-02-10T15:36:12.337098Z","shell.execute_reply":"2024-02-10T15:36:12.335884Z","shell.execute_reply.started":"2024-02-10T15:36:11.972916Z"}},"source":["## Primo tokenizer e modello usato\n","label_map = {label: idx for idx, label in enumerate(y.unique())}\n","y = y.map(label_map)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","print(len(label_map))\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(X_train)\n","X_train_seq = tokenizer.texts_to_sequences(X_train)\n","X_test_seq = tokenizer.texts_to_sequences(X_test)\n","\n","\n","max_len = 115 # lunghezza massima delle sequenze\n","X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n","X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n","\n","\n","\n","model = Sequential()\n","model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.5))) # Rimuovi GlobalMaxPooling1D\n","model.add(Dense(64, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(Dense(len(label_map), activation='softmax'))\n","\n","\n","\n","model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n","  \n","\n","history = model.fit(X_train_pad, y_train, epochs=50, batch_size=64, validation_data=(X_test_pad, y_test))\n","\n","\n","loss, accuracy = model.evaluate(X_test_pad, y_test)\n","print(\"Test Accuracy:\", accuracy)\n"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2024-02-10T09:09:35.601206Z","iopub.status.busy":"2024-02-10T09:09:35.600936Z","iopub.status.idle":"2024-02-10T09:09:47.929552Z","shell.execute_reply":"2024-02-10T09:09:47.928809Z","shell.execute_reply.started":"2024-02-10T09:09:35.601179Z"}},"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(df['text'])\n","sequences = tokenizer.texts_to_sequences(df['text'])\n","text_lengths = [len(sequence) for sequence in sequences]\n","\n","# Calcolo della lunghezza massima\n","max_length = max(text_lengths)\n","print(\"Lunghezza massima dei testi:\", max_length)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-02-10T16:01:45.063899Z","iopub.status.busy":"2024-02-10T16:01:45.063546Z","iopub.status.idle":"2024-02-10T16:01:45.069279Z","shell.execute_reply":"2024-02-10T16:01:45.068120Z","shell.execute_reply.started":"2024-02-10T16:01:45.063865Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import classification_report\n","from transformers import BertJapaneseTokenizer\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Conv1D, Dense, Dropout, Bidirectional, LSTM\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from kerastuner.tuners import RandomSearch\n","from transformers import BertTokenizer\n","from kerastuner.engine.hyperparameters import HyperParameters\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-10T17:09:07.768611Z","iopub.status.busy":"2024-02-10T17:09:07.768186Z","iopub.status.idle":"2024-02-10T17:09:10.225797Z","shell.execute_reply":"2024-02-10T17:09:10.224725Z","shell.execute_reply.started":"2024-02-10T17:09:07.768580Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_34/3129543821.py:17: DtypeWarning: Columns (2,3) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv(\"/kaggle/input/japanese-newspapers-20052021/japanese_news.csv\", delimiter='\\t')\n"]},{"name":"stdout","output_type":"stream","text":["Numero di valori unici nella colonna 'source': 21\n"]}],"source":["df = pd.read_csv(\"/kaggle/input/japanese-newspapers-20052021/japanese_news.csv\", delimiter='\\t')\n","df['text'] = df['text'].fillna('')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(df.head())\n","print(df.info())\n","print(df.columns)\n","\n","percentages = df['source'].value_counts(normalize=True) * 100\n","\n","plt.figure(figsize=(10, 6))\n","percentages.plot(kind='bar', color='skyblue')\n","plt.title('Percentage distribution for Source')\n","plt.xlabel('Source')\n","plt.ylabel('Percentage')\n","plt.xticks(rotation=45)\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","plt.tight_layout()\n","plt.show()\n","\n","source_counts = df['source'].value_counts()\n","\n","plt.figure(figsize=(10, 6))\n","sns.barplot(x=source_counts.index, y=source_counts.values, palette=\"viridis\")\n","plt.title('Source distribution')\n","plt.xlabel('Source')\n","plt.ylabel('Number of articles')\n","plt.xticks(rotation=45)\n","plt.show()\n","\n","\n","df['date'] = pd.to_datetime(df['date'], errors='coerce')\n","\n","invalid_dates = df[df['date'].isnull()]\n","\n","print(\"Rows with invalid dates:\")\n","print(invalid_dates)\n","\n","df = df.dropna(subset=['date'])\n","\n","df['year'] = df['date'].dt.year\n","\n","for year, year_data in df.groupby('year'):\n","    source_counts = year_data['source'].value_counts()\n","\n","    plt.figure(figsize=(10, 6))\n","    sns.barplot(x=source_counts.index, y=source_counts.values, hue=source_counts.index, palette=\"viridis\", legend=False)\n","    plt.title(f'Source distribution - Year {year}')\n","    plt.xlabel('Source')\n","    plt.ylabel('Number of articles')\n","    plt.xticks(rotation=45)\n","    plt.show()\n","    plt.tight_layout()\n","\n","df['year'] = pd.to_datetime(df['date']).dt.year\n","articles_per_year = df['year'].value_counts().sort_index()\n","\n","plt.figure(figsize=(10,6))\n","articles_per_year.plot(kind='line', marker='o')\n","plt.title('Number of articles per year')\n","plt.xlabel('Year')\n","plt.ylabel('Number of articles')\n","plt.xticks(rotation=45)\n","plt.grid(True)\n","plt.show()\n","plt.tight_layout()\n","\n","articles_per_source = df['source'].value_counts()\n","\n","plt.figure(figsize=(10,6))\n","articles_per_source.plot(kind='line', marker='o')\n","plt.title('Number of articles per source')\n","plt.xlabel('Source')\n","plt.ylabel('Number of articles')\n","plt.xticks(rotation=45)\n","plt.show()\n","plt.tight_layout()\n","\n","articles_per_year_and_source = df.groupby(['year', 'source']).size().unstack(fill_value=0)\n","\n","plt.figure(figsize=(18, 12))\n","articles_per_year_and_source.plot(kind='line', marker='o')\n","plt.title('Number of articles per year per source')\n","plt.xlabel('Year')\n","plt.ylabel('Number of articles')\n","plt.xticks(articles_per_year_and_source.index, rotation=45)\n","plt.grid(True)\n","plt.legend(title='Source', bbox_to_anchor=(1, 1))\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["desired_sample_size = 150000\n","df_sample = df.sample(n=desired_sample_size, random_state=42)\n","\n","X = df_sample['text']\n","y = df_sample['source']\n","\n","num_unique_sources = df_sample['source'].nunique()\n","print(\"Numero di valori unici nella colonna 'source':\", num_unique_sources)\n","\n","label_encoder = LabelEncoder()\n","y = label_encoder.fit_transform(y)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-02-10T17:09:14.384421Z","iopub.status.busy":"2024-02-10T17:09:14.383984Z","iopub.status.idle":"2024-02-10T17:14:33.090755Z","shell.execute_reply":"2024-02-10T17:14:33.089598Z","shell.execute_reply.started":"2024-02-10T17:09:14.384386Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8743cf50c7ee4bd7a5590de6b4b50423","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/104 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b010a13da894867b503725de8ef7c0f","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/258k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"46b24f93a38c4228bbcbc76de6437428","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/479 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n","The class this function is called from is 'BertTokenizer'.\n"]}],"source":["tokenizer = BertTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n","X_train_tok = tokenizer(X_train.tolist(), padding=True, truncation=True, max_length=115, return_tensors=\"tf\")\n","X_test_tok = tokenizer(X_test.tolist(), padding=True, truncation=True, max_length=115, return_tensors=\"tf\")\n","\n","max_len = 115  # Adjust according to the distribution of text lengths\n","X_train_pad = pad_sequences(X_train_tok[\"input_ids\"], maxlen=max_len, padding='post')\n","X_test_pad = pad_sequences(X_test_tok[\"input_ids\"], maxlen=max_len, padding='post')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-10T17:14:33.093176Z","iopub.status.busy":"2024-02-10T17:14:33.092781Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Trial 1 Complete [02h 01m 02s]\n","val_accuracy: 0.5042666792869568\n","\n","Best val_accuracy So Far: 0.5042666792869568\n","Total elapsed time: 02h 01m 02s\n","\n","Search: Running Trial #2\n","\n","Value             |Best Value So Far |Hyperparameter\n","300               |50                |embedding_output_dim\n","192               |128               |conv1d_filters\n","96                |96                |lstm_units\n","0.5               |0.2               |lstm_dropout\n","0.5               |0.4               |recurrent_dropout\n","64                |128               |dense_units\n","0.5               |0.2               |dropout\n","0.0001            |0.001             |learning_rate\n","\n","Epoch 1/5\n","1232/3750 [========>.....................] - ETA: 16:48 - loss: 2.6775 - accuracy: 0.1352"]},{"name":"stderr","output_type":"stream","text":["IOPub message rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_msg_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"]},{"name":"stdout","output_type":"stream","text":["3750/3750 [==============================] - 1496s 397ms/step - loss: 2.4378 - accuracy: 0.2176 - val_loss: 2.0536 - val_accuracy: 0.3409\n","Epoch 2/5\n","3750/3750 [==============================] - 1454s 388ms/step - loss: 2.0543 - accuracy: 0.3491 - val_loss: 1.9038 - val_accuracy: 0.3856\n","Epoch 3/5\n","2173/3750 [================>.............] - ETA: 9:48 - loss: 1.9280 - accuracy: 0.3899"]}],"source":["def build_model(hp):\n","    model = Sequential()\n","    model.add(Embedding(input_dim=len(tokenizer), output_dim=hp.Int('embedding_output_dim', min_value=50, max_value=300, step=50)))\n","    model.add(Conv1D(hp.Int('conv1d_filters', min_value=64, max_value=256, step=64), 5, activation='relu'))\n","    model.add(Bidirectional(LSTM(hp.Int('lstm_units', min_value=32, max_value=128, step=32), dropout=hp.Float('lstm_dropout', min_value=0.1, max_value=0.5, step=0.1), recurrent_dropout=hp.Float('recurrent_dropout', min_value=0.1, max_value=0.5, step=0.1))))\n","    model.add(Dense(hp.Int('dense_units', min_value=32, max_value=128, step=32), activation='relu'))\n","    model.add(Dropout(hp.Float('dropout', min_value=0.1, max_value=0.5, step=0.1)))\n","    model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n","\n","    model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])), metrics=['accuracy'])\n","    return model\n","\n","tuner = RandomSearch(\n","    build_model,\n","    objective='val_accuracy',\n","    max_trials=15,  \n","    executions_per_trial=1,  \n","    directory='my_dir',\n","    project_name='japanese_news'\n",")\n","\n","tuner.search(X_train_pad, y_train, epochs=5, validation_data=(X_test_pad, y_test))\n","\n","best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n","\n","model = tuner.hypermodel.build(best_hps)\n","history = model.fit(X_train_pad, y_train, epochs=30, batch_size=64, validation_data=(X_test_pad, y_test))\n","\n","loss, accuracy = model.evaluate(X_test_pad, y_test)\n","print(\"Test Accuracy:\", accuracy)\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":2237026,"sourceId":3748403,"sourceType":"datasetVersion"}],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
