{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3748403,"sourceType":"datasetVersion","datasetId":2237026}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install keras-tuner --upgrade","metadata":{"execution":{"iopub.status.busy":"2024-02-10T16:01:32.423694Z","iopub.execute_input":"2024-02-10T16:01:32.424312Z","iopub.status.idle":"2024-02-10T16:01:45.061306Z","shell.execute_reply.started":"2024-02-10T16:01:32.424279Z","shell.execute_reply":"2024-02-10T16:01:45.059967Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Requirement already satisfied: keras-tuner in /opt/conda/lib/python3.10/site-packages (1.4.6)\nRequirement already satisfied: keras in /opt/conda/lib/python3.10/site-packages (from keras-tuner) (2.15.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from keras-tuner) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from keras-tuner) (2.31.0)\nRequirement already satisfied: kt-legacy in /opt/conda/lib/python3.10/site-packages (from keras-tuner) (1.0.5)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->keras-tuner) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->keras-tuner) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->keras-tuner) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->keras-tuner) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->keras-tuner) (2023.11.17)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Primo tokenizer e modello usato\nlabel_map = {label: idx for idx, label in enumerate(y.unique())}\ny = y.map(label_map)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nprint(len(label_map))\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_test_seq = tokenizer.texts_to_sequences(X_test)\n\n\nmax_len = 115 # lunghezza massima delle sequenze\nX_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\nX_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n\n\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100))\nmodel.add(Conv1D(128, 5, activation='relu'))\nmodel.add(Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.5))) # Rimuovi GlobalMaxPooling1D\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(len(label_map), activation='softmax'))\n\n\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=tf.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n  \n\nhistory = model.fit(X_train_pad, y_train, epochs=50, batch_size=64, validation_data=(X_test_pad, y_test))\n\n\nloss, accuracy = model.evaluate(X_test_pad, y_test)\nprint(\"Test Accuracy:\", accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-10T15:36:11.972040Z","iopub.execute_input":"2024-02-10T15:36:11.972959Z","iopub.status.idle":"2024-02-10T15:36:12.337098Z","shell.execute_reply.started":"2024-02-10T15:36:11.972916Z","shell.execute_reply":"2024-02-10T15:36:12.335884Z"}}},{"cell_type":"markdown","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(df['text'])\nsequences = tokenizer.texts_to_sequences(df['text'])\ntext_lengths = [len(sequence) for sequence in sequences]\n\n# Calcolo della lunghezza massima\nmax_length = max(text_lengths)\nprint(\"Lunghezza massima dei testi:\", max_length)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T09:09:35.600936Z","iopub.execute_input":"2024-02-10T09:09:35.601206Z","iopub.status.idle":"2024-02-10T09:09:47.929552Z","shell.execute_reply.started":"2024-02-10T09:09:35.601179Z","shell.execute_reply":"2024-02-10T09:09:47.928809Z"}}},{"cell_type":"code","source":"import keras_tuner as kt\nfrom tensorflow import keras","metadata":{"execution":{"iopub.status.busy":"2024-02-10T16:01:45.063546Z","iopub.execute_input":"2024-02-10T16:01:45.063899Z","iopub.status.idle":"2024-02-10T16:01:45.069279Z","shell.execute_reply.started":"2024-02-10T16:01:45.063865Z","shell.execute_reply":"2024-02-10T16:01:45.068120Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report\nfrom transformers import BertJapaneseTokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Conv1D, Dense, Dropout, Bidirectional, LSTM\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom kerastuner.tuners import RandomSearch\nfrom transformers import BertTokenizer\nfrom kerastuner.engine.hyperparameters import HyperParameters\nimport matplotlib.pyplot as plt\n\n# Load data\ndf = pd.read_csv(\"/kaggle/input/japanese-newspapers-20052021/japanese_news.csv\", delimiter='\\t')\ndf['text'] = df['text'].fillna('')\n\n\n# Sample data\ndesired_sample_size = 150000\ndf_sample = df.sample(n=desired_sample_size, random_state=42)\n\n# Prepare data\nX = df_sample['text']\ny = df_sample['source']\n\nnum_unique_sources = df_sample['source'].nunique()\nprint(\"Numero di valori unici nella colonna 'source':\", num_unique_sources)\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T17:09:07.768186Z","iopub.execute_input":"2024-02-10T17:09:07.768611Z","iopub.status.idle":"2024-02-10T17:09:10.225797Z","shell.execute_reply.started":"2024-02-10T17:09:07.768580Z","shell.execute_reply":"2024-02-10T17:09:10.224725Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/3129543821.py:17: DtypeWarning: Columns (2,3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(\"/kaggle/input/japanese-newspapers-20052021/japanese_news.csv\", delimiter='\\t')\n","output_type":"stream"},{"name":"stdout","text":"Numero di valori unici nella colonna 'source': 21\n","output_type":"stream"}]},{"cell_type":"code","source":"# Tokenization using pre-trained tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\nX_train_tok = tokenizer(X_train.tolist(), padding=True, truncation=True, max_length=115, return_tensors=\"tf\")\nX_test_tok = tokenizer(X_test.tolist(), padding=True, truncation=True, max_length=115, return_tensors=\"tf\")\n\n# Padding sequences\nmax_len = 115  # Adjust according to the distribution of text lengths\nX_train_pad = pad_sequences(X_train_tok[\"input_ids\"], maxlen=max_len, padding='post')\nX_test_pad = pad_sequences(X_test_tok[\"input_ids\"], maxlen=max_len, padding='post')\n","metadata":{"execution":{"iopub.status.busy":"2024-02-10T17:09:14.383984Z","iopub.execute_input":"2024-02-10T17:09:14.384421Z","iopub.status.idle":"2024-02-10T17:14:33.090755Z","shell.execute_reply.started":"2024-02-10T17:09:14.384386Z","shell.execute_reply":"2024-02-10T17:14:33.089598Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/104 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8743cf50c7ee4bd7a5590de6b4b50423"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/258k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b010a13da894867b503725de8ef7c0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/479 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46b24f93a38c4228bbcbc76de6437428"}},"metadata":{}},{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \nThe class this function is called from is 'BertTokenizer'.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define model creation function\ndef build_model(hp):\n    model = Sequential()\n    model.add(Embedding(input_dim=len(tokenizer), output_dim=hp.Int('embedding_output_dim', min_value=50, max_value=300, step=50)))\n    model.add(Conv1D(hp.Int('conv1d_filters', min_value=64, max_value=256, step=64), 5, activation='relu'))\n    model.add(Bidirectional(LSTM(hp.Int('lstm_units', min_value=32, max_value=128, step=32), dropout=hp.Float('lstm_dropout', min_value=0.1, max_value=0.5, step=0.1), recurrent_dropout=hp.Float('recurrent_dropout', min_value=0.1, max_value=0.5, step=0.1))))\n    model.add(Dense(hp.Int('dense_units', min_value=32, max_value=128, step=32), activation='relu'))\n    model.add(Dropout(hp.Float('dropout', min_value=0.1, max_value=0.5, step=0.1)))\n    model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n\n    model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])), metrics=['accuracy'])\n    return model\n\n# Instantiate the tuner and perform random search\ntuner = RandomSearch(\n    build_model,\n    objective='val_accuracy',\n    max_trials=5,  # Number of hyperparameter combinations to try\n    executions_per_trial=1,  # Number of models to train and evaluate for each trial\n    directory='my_dir',\n    project_name='japanese_news'\n)\n\ntuner.search(X_train_pad, y_train, epochs=5, validation_data=(X_test_pad, y_test))\n\n# Get the best hyperparameters\nbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n\n# Build the model with the best hyperparameters and train it\nmodel = tuner.hypermodel.build(best_hps)\nhistory = model.fit(X_train_pad, y_train, epochs=30, batch_size=64, validation_data=(X_test_pad, y_test))\n\n# Evaluate model\nloss, accuracy = model.evaluate(X_test_pad, y_test)\nprint(\"Test Accuracy:\", accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-10T17:14:33.092781Z","iopub.execute_input":"2024-02-10T17:14:33.093176Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Trial 1 Complete [02h 01m 02s]\nval_accuracy: 0.5042666792869568\n\nBest val_accuracy So Far: 0.5042666792869568\nTotal elapsed time: 02h 01m 02s\n\nSearch: Running Trial #2\n\nValue             |Best Value So Far |Hyperparameter\n300               |50                |embedding_output_dim\n192               |128               |conv1d_filters\n96                |96                |lstm_units\n0.5               |0.2               |lstm_dropout\n0.5               |0.4               |recurrent_dropout\n64                |128               |dense_units\n0.5               |0.2               |dropout\n0.0001            |0.001             |learning_rate\n\nEpoch 1/5\n1232/3750 [========>.....................] - ETA: 16:48 - loss: 2.6775 - accuracy: 0.1352","output_type":"stream"},{"text":"IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"},{"name":"stdout","text":"3750/3750 [==============================] - 1496s 397ms/step - loss: 2.4378 - accuracy: 0.2176 - val_loss: 2.0536 - val_accuracy: 0.3409\nEpoch 2/5\n3750/3750 [==============================] - 1454s 388ms/step - loss: 2.0543 - accuracy: 0.3491 - val_loss: 1.9038 - val_accuracy: 0.3856\nEpoch 3/5\n2173/3750 [================>.............] - ETA: 9:48 - loss: 1.9280 - accuracy: 0.3899","output_type":"stream"}]},{"cell_type":"markdown","source":"#New Tokenizer without finetuining\ntokenizer = BertTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\nX_train_tok = tokenizer(X_train.tolist(), padding=True, truncation=True, max_length=115, return_tensors=\"tf\")\nX_test_tok = tokenizer(X_test.tolist(), padding=True, truncation=True, max_length=115, return_tensors=\"tf\")\n\n\nmax_len = 115  # Adjust according to the distribution of text lengths\nX_train_pad = pad_sequences(X_train_tok[\"input_ids\"], maxlen=max_len, padding='post')\nX_test_pad = pad_sequences(X_test_tok[\"input_ids\"], maxlen=max_len, padding='post')\n\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=len(tokenizer), output_dim=100))\nmodel.add(Conv1D(128, 5, activation='relu'))\nmodel.add(Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.5)))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(len(label_encoder.classes_), activation='softmax'))\n\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=tf.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n\nhistory = model.fit(X_train_pad, y_train, epochs=30, batch_size=64, validation_data=(X_test_pad, y_test))\n\nloss, accuracy = model.evaluate(X_test_pad, y_test)\nprint(\"Test Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T09:22:44.678341Z","iopub.execute_input":"2024-02-10T09:22:44.678630Z"}}}]}