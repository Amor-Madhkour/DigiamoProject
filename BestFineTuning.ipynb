{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install keras-tuner --upgrade\n","!pip install transformers\n","!pip install fugashi\n"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2024-02-10T15:36:11.972959Z","iopub.status.busy":"2024-02-10T15:36:11.972040Z","iopub.status.idle":"2024-02-10T15:36:12.337098Z","shell.execute_reply":"2024-02-10T15:36:12.335884Z","shell.execute_reply.started":"2024-02-10T15:36:11.972916Z"}},"source":["## Primo tokenizer e modello usato\n","label_map = {label: idx for idx, label in enumerate(y.unique())}\n","y = y.map(label_map)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","print(len(label_map))\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(X_train)\n","X_train_seq = tokenizer.texts_to_sequences(X_train)\n","X_test_seq = tokenizer.texts_to_sequences(X_test)\n","\n","\n","max_len = 115 # lunghezza massima delle sequenze\n","X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n","X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n","\n","\n","\n","model = Sequential()\n","model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.5))) # Rimuovi GlobalMaxPooling1D\n","model.add(Dense(64, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(Dense(len(label_map), activation='softmax'))\n","\n","\n","\n","model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n","  \n","\n","history = model.fit(X_train_pad, y_train, epochs=50, batch_size=64, validation_data=(X_test_pad, y_test))\n","\n","\n","loss, accuracy = model.evaluate(X_test_pad, y_test)\n","print(\"Test Accuracy:\", accuracy)\n"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2024-02-10T09:09:35.601206Z","iopub.status.busy":"2024-02-10T09:09:35.600936Z","iopub.status.idle":"2024-02-10T09:09:47.929552Z","shell.execute_reply":"2024-02-10T09:09:47.928809Z","shell.execute_reply.started":"2024-02-10T09:09:35.601179Z"}},"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(df['text'])\n","sequences = tokenizer.texts_to_sequences(df['text'])\n","text_lengths = [len(sequence) for sequence in sequences]\n","\n","# Calcolo della lunghezza massima\n","max_length = max(text_lengths)\n","print(\"Lunghezza massima dei testi:\", max_length)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","import seaborn as sns\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import classification_report\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Conv1D, Dense, Dropout, Bidirectional, LSTM\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras_tuner.tuners import Hyperband\n","from transformers import BertTokenizer\n","import matplotlib.pyplot as plt\n","\n","# Load data\n","df = pd.read_csv(\"/kaggle/input/japanese-newspapers-20052021/japanese_news.csv\", delimiter='\\t')\n","df['text'] = df['text'].fillna('')\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(df.head())\n","print(df.info())\n","print(df.columns)\n","\n","\n","# Conta il numero di campioni per ogni elemento nella colonna 'source'\n","source_counts = data['source'].value_counts()\n","\n","# Stampare il numero di campioni per ogni elemento\n","print(\"Numero di campioni per ciascun elemento nella colonna 'source':\")\n","print(source_counts)\n","percentages = df['source'].value_counts(normalize=True) * 100\n","\n","plt.figure(figsize=(10, 6))\n","percentages.plot(kind='bar', color='skyblue')\n","plt.title('Percentage distribution for {}'.format(source_column))\n","plt.xlabel(source_column)\n","plt.ylabel('Percentage')\n","plt.xticks(rotation=45)\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","plt.tight_layout()\n","plt.show()\n","\n","source_counts = df['source'].value_counts()\n","\n","plt.figure(figsize=(10, 6))\n","sns.barplot(x=source_counts.index, y=source_counts.values, palette=\"viridis\")\n","plt.title('Source distribution')\n","plt.xlabel('Source')\n","plt.ylabel('Number of articles')\n","plt.xticks(rotation=45)\n","plt.show()\n","\n","\n","df['date'] = pd.to_datetime(df['date'], errors='coerce')\n","\n","invalid_dates = df[df['date'].isnull()]\n","\n","print(\"Rows with invalid dates:\")\n","print(invalid_dates)\n","\n","df = df.dropna(subset=['date'])\n","\n","df['year'] = df['date'].dt.year\n","\n","for year, year_data in df.groupby('year'):\n","    source_counts = year_data['source'].value_counts()\n","\n","    plt.figure(figsize=(10, 6))\n","    sns.barplot(x=source_counts.index, y=source_counts.values, hue=source_counts.index, palette=\"viridis\", legend=False)\n","    plt.title(f'Source distribution - Year {year}')\n","    plt.xlabel('Source')\n","    plt.ylabel('Number of articles')\n","    plt.xticks(rotation=45)\n","    plt.show()\n","    plt.tight_layout()\n","\n","df['year'] = pd.to_datetime(df['date']).dt.year\n","articles_per_year = df['year'].value_counts().sort_index()\n","\n","plt.figure(figsize=(10,6))\n","articles_per_year.plot(kind='line', marker='o')\n","plt.title('Number of articles per year')\n","plt.xlabel('Year')\n","plt.ylabel('Number of articles')\n","plt.xticks(rotation=45)\n","plt.grid(True)\n","plt.show()\n","plt.tight_layout()\n","\n","articles_per_source = df['source'].value_counts()\n","\n","plt.figure(figsize=(10,6))\n","articles_per_source.plot(kind='line', marker='o')\n","plt.title('Number of articles per source')\n","plt.xlabel('Source')\n","plt.ylabel('Number of articles')\n","plt.xticks(rotation=45)\n","plt.show()\n","plt.tight_layout()\n","\n","articles_per_year_and_source = df.groupby(['year', 'source']).size().unstack(fill_value=0)\n","\n","plt.figure(figsize=(18, 12))\n","articles_per_year_and_source.plot(kind='line', marker='o')\n","plt.title('Number of articles per year per source')\n","plt.xlabel('Year')\n","plt.ylabel('Number of articles')\n","plt.xticks(articles_per_year_and_source.index, rotation=45)\n","plt.grid(True)\n","plt.legend(title='Source', bbox_to_anchor=(1, 1))\n","plt.tight_layout()\n","plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["desired_sample_size = 50000\n","df_sample = df.sample(n=desired_sample_size, random_state=42)\n","\n","X = df_sample['text']\n","y = df_sample['source']\n","\n","num_unique_sources = df_sample['source'].nunique()\n","print(\"Numero di valori unici nella colonna 'source':\", num_unique_sources)\n","\n","label_encoder = LabelEncoder()\n","y = label_encoder.fit_transform(y)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","tokenizer = BertTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n","X_train_tok = tokenizer(X_train.tolist(), padding=True, truncation=True, max_length=115, return_tensors=\"tf\")\n","X_test_tok = tokenizer(X_test.tolist(), padding=True, truncation=True, max_length=115, return_tensors=\"tf\")\n","\n","max_len = 115  \n","X_train_pad = pad_sequences(X_train_tok[\"input_ids\"], maxlen=max_len, padding='post')\n","X_test_pad = pad_sequences(X_test_tok[\"input_ids\"], maxlen=max_len, padding='post')\n","\n","def build_model(hp):\n","    model = Sequential()\n","    model.add(Embedding(input_dim=len(tokenizer.vocab), output_dim=hp.Int('embedding_output_dim', min_value=100, max_value=300, step=100)))\n","    model.add(Conv1D(hp.Int('conv1d_filters', min_value=64, max_value=256, step=64), 5, activation='relu'))\n","    model.add(Bidirectional(LSTM(hp.Int('lstm_units', min_value=32, max_value=128, step=32), dropout=hp.Float('lstm_dropout', min_value=0.1, max_value=0.5, step=0.1), recurrent_dropout=hp.Float('recurrent_dropout', min_value=0.1, max_value=0.5, step=0.1))))\n","    model.add(Dense(hp.Int('dense_units', min_value=32, max_value=128, step=32), activation='relu'))\n","    model.add(Dropout(hp.Float('dropout', min_value=0.1, max_value=0.4, step=0.1)))\n","    model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n","\n","    model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])), metrics=['accuracy'])\n","    return model\n","\n","\n","tuner = Hyperband(\n","    build_model,\n","    objective='val_accuracy',\n","    max_epochs=30,  \n","    factor=3,  \n","    directory='my_dir',\n","    project_name='japanese_news'\n",")\n","\n","tuner.search(X_train_pad, y_train, epochs=30, validation_data=(X_test_pad, y_test))\n","\n","best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n","\n","model = tuner.hypermodel.build(best_hps)\n","history = model.fit(X_train_pad, y_train, epochs=30, batch_size=64, validation_data=(X_test_pad, y_test))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Evaluate model\n","loss, accuracy = model.evaluate(X_test_pad, y_test)\n","print(\"Test Accuracy:\", accuracy)\n","\n","# Fai previsioni sul set di test\n","y_pred = model.predict_classes(X_test_pad)\n","\n","# Calcola l'F1-score e la matrice di confusione\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Confusion Matrix:\")\n","conf_mat = confusion_matrix(y_test, y_pred)\n","print(conf_mat)\n","# Plot confusion matrix\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Define model creation function\n","def build_model(hp):\n","    model = Sequential()\n","    model.add(Embedding(input_dim=len(tokenizer), output_dim=hp.Int('embedding_output_dim', min_value=100, max_value=300, step=50)))\n","    model.add(Conv1D(hp.Int('conv1d_filters', min_value=64, max_value=256, step=64), 5, activation='relu'))\n","    model.add(Bidirectional(LSTM(hp.Int('lstm_units', min_value=32, max_value=128, step=32), dropout=hp.Float('lstm_dropout', min_value=0.1, max_value=0.5, step=0.1), recurrent_dropout=hp.Float('recurrent_dropout', min_value=0.1, max_value=0.5, step=0.1))))\n","    model.add(Dense(hp.Int('dense_units', min_value=32, max_value=128, step=32), activation='relu'))\n","    model.add(Dropout(hp.Float('dropout', min_value=0.1, max_value=0.5, step=0.1)))\n","    model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n","\n","    model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])), metrics=['accuracy'])\n","    return model\n","\n","# Instantiate the tuner and perform random search\n","tuner = RandomSearch(\n","    build_model,\n","    objective='val_accuracy',\n","    max_trials=5,  # Number of hyperparameter combinations to try\n","    executions_per_trial=1,  # Number of models to train and evaluate for each trial\n","    directory='my_dir',\n","    project_name='japanese_news'\n",")\n","\n","tuner.search(X_train_pad, y_train, epochs=5, validation_data=(X_test_pad, y_test))\n","\n","# Get the best hyperparameters\n","best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n","\n","# Build the model with the best hyperparameters and train it\n","model = tuner.hypermodel.build(best_hps)\n","history = model.fit(X_train_pad, y_train, epochs=30, batch_size=64, validation_data=(X_test_pad, y_test))\n","\n","# Evaluate model\n","loss, accuracy = model.evaluate(X_test_pad, y_test)\n","print(\"Test Accuracy:\", accuracy)\n"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2024-02-10T09:22:44.678630Z","iopub.status.busy":"2024-02-10T09:22:44.678341Z"}},"source":["#New Tokenizer without finetuining\n","tokenizer = BertTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n","X_train_tok = tokenizer(X_train.tolist(), padding=True, truncation=True, max_length=115, return_tensors=\"tf\")\n","X_test_tok = tokenizer(X_test.tolist(), padding=True, truncation=True, max_length=115, return_tensors=\"tf\")\n","\n","\n","max_len = 115  # Adjust according to the distribution of text lengths\n","X_train_pad = pad_sequences(X_train_tok[\"input_ids\"], maxlen=max_len, padding='post')\n","X_test_pad = pad_sequences(X_test_tok[\"input_ids\"], maxlen=max_len, padding='post')\n","\n","\n","model = Sequential()\n","model.add(Embedding(input_dim=len(tokenizer), output_dim=100))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.5)))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n","\n","\n","model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n","\n","history = model.fit(X_train_pad, y_train, epochs=30, batch_size=64, validation_data=(X_test_pad, y_test))\n","\n","loss, accuracy = model.evaluate(X_test_pad, y_test)\n","print(\"Test Accuracy:\", accuracy)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":2237026,"sourceId":3748403,"sourceType":"datasetVersion"}],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
